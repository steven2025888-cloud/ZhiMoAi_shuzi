[MODEL] 正在加载 IndexTTS2 声学模型...
>> GPT weights restored from: D:\ZhiMoAi_shuzi\IndexTTS2-SonicVale\checkpoints\gpt.pth
GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From \U0001f449v4.50\U0001f448 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
>> Failed to load custom CUDA kernel for BigVGAN. Falling back to torch.
TypeError("unsupported operand type(s) for +: 'NoneType' and 'str'")
>> semantic_codec weights restored from: D:\ZhiMoAi_shuzi\IndexTTS2-SonicVale\checkpoints\hf_cache\models--amphion--MaskGCT\snapshots\265c6cef07625665d0c28d2faafb1415562379dc\semantic_codec\model.safetensors
cfm loaded
length_regulator loaded
gpt_layer loaded
>> s2mel weights restored from: D:\ZhiMoAi_shuzi\IndexTTS2-SonicVale\checkpoints\s2mel.pth
>> campplus_model weights restored from: D:\ZhiMoAi_shuzi\IndexTTS2-SonicVale\checkpoints\hf_cache\models--funasr--campplus\snapshots\fb71fe990cbf6031ae6987a2d76fe64f94377b7e\campplus_cn_common.bin
Loading weights from nvidia/bigvgan_v2_22khz_80band_256x
Removing weight norm...
>> bigvgan weights restored from: nvidia/bigvgan_v2_22khz_80band_256x
>> TextNormalizer loaded
>> bpe model loaded from: D:\ZhiMoAi_shuzi\IndexTTS2-SonicVale\checkpoints\bpe.model
[MODEL] 模型加载完成，正在预热引擎...
>> starting inference...
Use the specified emotion vector
The following generation flags are not valid and may be ignored: ['length_penalty']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.53.0. You should pass an instance of `Cache` instead, e.g. `past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.
  0%|          | 0/25 [00:00<?, ?it/s]  4%|         | 1/25 [00:00<00:03,  7.21it/s] 28%|       | 7/25 [00:00<00:00, 31.71it/s] 44%|     | 11/25 [00:00<00:00, 25.48it/s] 56%|    | 14/25 [00:00<00:00, 23.75it/s] 68%|   | 17/25 [00:00<00:00, 22.50it/s] 80%|  | 20/25 [00:00<00:00, 21.97it/s] 92%|| 23/25 [00:01<00:00, 21.65it/s]100%|| 25/25 [00:01<00:00, 22.28it/s]
torch.Size([1, 18432])
>> gpt_gen_time: 1.04 seconds
>> gpt_forward_time: 0.01 seconds
>> s2mel_time: 1.13 seconds
>> bigvgan_time: 0.28 seconds
>> Total inference time: 3.89 seconds
>> Generated audio length: 0.84 seconds
>> RTF: 4.6522
>> wav file saved to: D:\ZhiMoAi_shuzi\unified_outputs\_warmup.wav
[MODEL] 引擎预热完成，首次合成将直接输出
[MODEL] OK
[WARMUP] 正在预热 LatentSync 引擎...
[WARMUP] LatentSync 预热返回非零码: 1
[WARMUP] stderr: Traceback (most recent call last):
  File "<string>", line 1, in <module>
ImportError: cannot import name 'load_model' from 'latentsync.utils.util' (D:\ZhiMoAi_shuzi\LatentSync\latentsync\utils\util.py)

